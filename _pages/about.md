---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
- /about/
- /about.html
---
 
I am a second-year **Ph.D. student** in Computer Science at [Institute of Computing Technology, Chinese Academy of Sciences](http://www.ict.ac.cn), advised by Prof. [Shenghua Liu](https://shenghua-liu.github.io) and [Yiwei Wang](https://wangywust.github.io/). I am a member of [CAS Key Laboratory of AI Security](https://ict.cas.cn/jssgk/zzjg/kyxt/sjzn/js/). My research interests lie in the general area of trustworthy large language models. 

I completed a research internship at [MSRA GenAI](https://www.microsoft.com/en-us/research/group/general-artificial-intelligence/), focusing on post-training for LLMs.

I will be visiting the [Language Technologies Institute (LTI)](https://www.lti.cs.cmu.edu/), [Carnegie Mellon University (CMU)](https://www.cmu.edu/) this year for a one-year research stay under the supervision of Prof. [Chenyan Xiong](https://www.cs.cmu.edu/~cx/).


##  **üî• News**
- `2025-01`: Three papers accepted to *ICLR 2025*, *WWW 2025*, and *NAACL 2025*.
- `2024-11`: One paper accepted to *COLING 2025*
- `2024-09`: Two papers accepted to *EMNLP 2024*
- `2024-05`: One paper accepted to *ACL 2024*

## **üìö Publications**
{% if site.author.googlescholar %}
<div class="wordwrap">You can also find my articles on <a href="{{site.author.googlescholar}}">my Google Scholar profile</a>.</div>
{% endif %}

### 2025

- Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, `BaoLong Bi`, Ling-Rui Mei, Junfeng Fang, Zhijiang Guo, Le Song, Cheng-Lin Liu. **From System 1 to System 2: A Survey of Reasoning Large Language Models**. [[arxiv]](https://arxiv.org/abs/2502.17419) [[paper]](https://arxiv.org/pdf/2502.17419) [[github]](https://github.com/zzli2022/Awesome-System2-Reasoning-LLM)

- Cheng Wang, Yue Liu, `Baolong Bi`, Duzhen Zhang, Zhongzhi Li, Junfeng Fang. **Safety in Large Reasoning Models: A Survey**. [[arxiv]](https://arxiv.org/abs/2504.17704) [[paper]](https://arxiv.org/pdf/2504.17704)

- Lingrui Mei, Shenghua Liu, Yiwei Wang, `Baolong Bi`, Yuyao Ge, Jun Wan, Yurong Wu, Xueqi Cheng. **a1: Steep Test-time Scaling Law via Environment Augmented Generation**. [[arxiv]](https://arxiv.org/abs/2504.14597) [[paper]](https://arxiv.org/pdf/2504.14597)

- Yilong Xu, Jinhua Gao, Xiaoming Yu, Yuanhai Xue, `Baolong Bi`, Huawei Shen, Xueqi Cheng. **Training a Utility-based Retriever Through Shared Context Attribution for Retrieval-Augmented Language Models**. [[arxiv]](https://arxiv.org/abs/2504.00573) [[paper]](https://arxiv.org/pdf/2504.00573)

- Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, `Baolong Bi`, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi. **Efficient Inference for Large Reasoning Models: A Survey**. [[arxiv]](https://arxiv.org/abs/2503.23077) [[paper]](https://arxiv.org/pdf/2503.23077)

- Hongcheng Gao, Jiashu Qu, Jingyi Tang, `Baolong Bi`, Yue Liu, Hongyu Chen, Li Liang, Li Su, Qingming Huang. **Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation**. [[arxiv]](https://arxiv.org/abs/2503.19622) [paper](https://arxiv.org/pdf/2503.19622) [[huggingface]](https://huggingface.co/papers/2503.19622) [[github]](https://github.com/Hongcheng-Gao/HAVEN/tree/main)

- Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Lizhe Chen, `Baolong Bi`, Xueqi Cheng. **Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking**. [[arxiv]](https://arxiv.org/abs/2503.19602) [[paper]](https://arxiv.org/pdf/2503.19602)

- `Baolong Bi`, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui Mei, Xueqi Cheng. **Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models**.
[[arxiv]](https://arxiv.org/abs/2503.15888) [[paper]](https://arxiv.org/pdf/2503.15888) [[github]](https://github.com/byronBBL/CK-PLUG)

- Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, `Baolong Bi`, Xueqi Cheng. **Towards Fully Exploiting LLM Internal States to Enhance Knowledge
Boundary Perception**. [[arxiv]](https://arxiv.org/abs/2502.11677) [[paper]](https://arxiv.org/pdf/2502.11677)

- Zherui Li, Houcheng Jiang, Hao Chen, `Baolong Bi`, Zhenhong Zhou, Fei Sun, Junfeng Fang, Xiang Wang. **Reinforced Lifelong Editing for Language Models**. [[arxiv]](https://arxiv.org/abs/2502.05759) [[paper]](https://arxiv.org/pdf/2502.05759)

- Tianyu Zhang, Junfeng Fang, Houcheng Jiang, `Baolong Bi`, Xiang Wang, Xiangnan He. **Explainable and Efficient Editing for Large Language Models**. [[arxiv]](https://openreview.net/forum?id=iAn7rlIfgc#discussion) [[paper]](https://openreview.net/pdf?id=iAn7rlIfgc)

### 2024
- `Baolong Bi`, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui Mei, Junfeng Fang, Zehao Li, Furu Wei. **Context-DPO: Aligning Language Models for Context-Faithfulness**. [[page]](https://byronbbl.github.io/context-dpo.io/) [[arxiv]](https://www.arxiv.org/abs/2412.15280) [[paper]](https://arxiv.org/pdf/2412.15280)

-	Zehao Li, Wenwei Han, Yujun Cai, Hao Jiang, `Baolong Bi`, Shuqin Gao, Honglong Zhao, Zhaoqi Wang. **GradiSeg: Gradient-Guided Gaussian Segmentation with Enhanced 3D Boundary Precision**.
[[arxiv]](https://arxiv.org/abs/2412.00392) [[paper]](https://arxiv.org/pdf/2412.00392)

-	Yuyao Ge, Shenghua Liu, `Baolong Bi`, Yiwei Wang, Lingrui Mei, Wenjie Feng, Lizhe Chen, Xueqi Cheng. **Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?**.
[[arxiv]](https://arxiv.org/abs/2402.07140) [[paper]](https://arxiv.org/pdf/2402.07140v4)

-	Lingrui Mei, Shenghua Liu, Yiwei Wang, `Baolong Bi`, Ruibin Yuan, Xueqi Cheng. **HiddenGuard: Fine-Grained Safe Generation with Specialized Representation Router**.
[[arxiv]](https://arxiv.org/abs/2410.02684) [[paper]](https://arxiv.org/pdf/2410.02684)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, Xueqi Cheng. **StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models**.
[[arxiv]](https://arxiv.org/abs/2409.10132) [[paper]](https://arxiv.org/pdf/2409.10132)

-	Yilong Xu, Jinhua Gao, Xiaoming Yu, `Baolong Bi`, Huawei Shen, Xueqi Cheng. **ALiiCE: Evaluating Positional Fine-grained Citation Generation**.
[[arxiv]](https://arxiv.org/abs/2406.13375) [[paper]](https://arxiv.org/pdf/2406.13375)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, Xueqi Cheng. **Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities**.
[[arxiv]](https://arxiv.org/abs/2406.12468) [[paper]](https://arxiv.org/pdf/2406.12468)

-	Lingrui Mei, Shenghua Liu, Yiwei Wang, `Baolong Bi`, Jiayi Mao, Xueqi Cheng. **"Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak**.
[[arxiv]](https://arxiv.org/abs/2406.11668) [[paper]](https://arxiv.org/pdf/2406.11668)

-	`Baolong Bi`, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, Xueqi Cheng. **Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts**.
[[page]](https://deck-llm.meirtz.com/) [[arxiv]](https://arxiv.org/abs/2405.11613) [[paper]](https://arxiv.org/pdf/2405.11613.pdf)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng. **Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness**.
[[arxiv]](https://arxiv.org/abs/2404.00216) [[paper]](https://arxiv.org/pdf/2404.00216.pdf)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng. **LPNL: Scalable Link Prediction with Large Language Models**.
[[arxiv]](https://arxiv.org/abs/2401.13227) [[paper]](https://arxiv.org/pdf/2401.13227.pdf)

-	Lingrui Mei, Shenghua Liu, Yiwei Wang, `Baolong Bi`, Xueqi Cheng. **SLANG: New Concept Comprehension of Large Language Models**.
[[arxiv]](https://arxiv.org/abs/2401.12585) [[paper]](https://arxiv.org/pdf/2401.12585.pdf)

## **üìù Services**
- **Reviewer for**  
  - International Conference on Learning Representations (ICLR) 2025 
  - Annual Meeting of the Association for Computational Linguistics (ACL) 2024, 2025
    (ARR 2024 December; ARR 2025 February, May) 
  - Conference on Empirical Language Modeling (COLM) 2025
  - Conference on Neural Information Processing Systems (NeurIPS) 2025
  - Conference on NeurIPS Datasets and Benchmarks Track 2025

## **üéì Education**
- **Institute of Computing Technology, Chinese Academy of Sciences**  
  *Ph.D. in Computer Science* (2023 - present)  
  Advisor: Prof. [Shenghua Liu](https://shenghua-liu.github.io) and [Yiwei Wang](https://wangywust.github.io/)

- **Chongqing University**  
  *B.E. in Computer Science (Excellent)* (2019 - 2023)  
  Advisor: Prof. [Chengliang Wang](http://www.cs.cqu.edu.cn/info/1352/4177.htm)

## **üèÜ Honors and Awards**
  - 2021 Bronze medal in ICPC China National Programming Contest
  - 2020 The first prize of the National College Student Mathematical Modeling Contest
  - 2020 First prize in Chongqing University Student Programming Competition
  - 2017 First prize in the national youth informatics competition
  - Advanced Individual in Innovation and Entrepreneurship of Chongqing University
  - Ministry of Education-Huawei Smart Base "Future Star"
  - Chongqing University First Class Scholarship
