---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

{% if site.author.googlescholar %}
  <div class="wordwrap">You can also find my articles on <a href="{{site.author.googlescholar}}">my Google Scholar profile</a>.</div>
{% endif %}

<div class='paper-box-text' markdown="1">

### 2025

- Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, `Baolong Bi`, Xueqi Cheng **Towards Fully Exploiting LLM Internal States to Enhance Knowledge
Boundary Perception**. [[arxiv]](https://arxiv.org/abs/2502.11677) [[paper]](https://arxiv.org/pdf/2502.11677)

- Zherui Li, Houcheng Jiang, Hao Chen, `Baolong Bi`, Zhenhong Zhou, Fei Sun, Junfeng Fang, Xiang Wang. **Reinforced Lifelong Editing for Language Models**. [[arxiv]](https://arxiv.org/abs/2502.05759) [[paper]]https://arxiv.org/pdf/2502.05759)

- Tianyu Zhang, Junfeng Fang, Houcheng Jiang, `Baolong Bi`, Xiang Wang, Xiangnan He **Explainable and Efficient Editing for Large Language Models**. [[arxiv]](https://openreview.net/forum?id=iAn7rlIfgc#discussion) [[paper]](https://openreview.net/pdf?id=iAn7rlIfgc)

### 2024

- `Baolong Bi`, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui Mei, Junfeng Fang, Zehao Li, Furu Wei. **Context-DPO: Aligning Language Models for Context-Faithfulness**. [[arxiv]](https://www.arxiv.org/abs/2412.15280) [[paper]](https://arxiv.org/pdf/2412.15280)

-	Zehao Li, Wenwei Han, Yujun Cai, Hao Jiang, `Baolong Bi`, Shuqin Gao, Honglong Zhao, Zhaoqi Wang. **GradiSeg: Gradient-Guided Gaussian Segmentation with Enhanced 3D Boundary Precision**.
[[arxiv]](https://arxiv.org/abs/2412.00392) [[paper]](https://arxiv.org/pdf/2412.00392)

-	Yuyao Ge, Shenghua Liu, `Baolong Bi`, Yiwei Wang, Lingrui Mei, Wenjie Feng, Lizhe Chen, Xueqi Cheng. **Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?**.
[[arxiv]](https://arxiv.org/abs/2402.07140) [[paper]](https://arxiv.org/pdf/2402.07140v4)

-	Lingrui Mei, Shenghua Liu, Yiwei Wang, `Baolong Bi`, Ruibin Yuan, Xueqi Chengã€‚ **HiddenGuard: Fine-Grained Safe Generation with Specialized Representation Router**.
[[arxiv]](https://arxiv.org/abs/2410.02684) [[paper]](https://arxiv.org/pdf/2410.02684)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, Xueqi Cheng. **StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models**.
[[arxiv]](https://arxiv.org/abs/2409.10132) [[paper]](https://arxiv.org/pdf/2409.10132)

-	Yilong Xu, Jinhua Gao, Xiaoming Yu, `Baolong Bi`, Huawei Shen, Xueqi Cheng. **ALiiCE: Evaluating Positional Fine-grained Citation Generation**.
[[arxiv]](https://arxiv.org/abs/2406.13375) [[paper]](https://arxiv.org/pdf/2406.13375)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, Xueqi Cheng. **Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities**.
[[arxiv]](https://arxiv.org/abs/2406.12468) [[paper]](https://arxiv.org/pdf/2406.12468)

-	Lingrui Mei, Shenghua Liu, Yiwei Wang, `Baolong Bi`, Jiayi Mao, Xueqi Cheng. **"Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak**.
[[arxiv]](https://arxiv.org/abs/2406.11668) [[paper]](https://arxiv.org/pdf/2406.11668)

-	`Baolong Bi`, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, Xueqi Cheng. **Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts**.
[[page]](https://deck-llm.meirtz.com/) [[arxiv]](https://arxiv.org/abs/2405.11613) [[paper]](https://arxiv.org/pdf/2405.11613.pdf)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng. **Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark**.
[[arxiv]](https://arxiv.org/abs/2404.00216) [[paper]](https://arxiv.org/pdf/2404.00216.pdf)

-	`Baolong Bi`, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng. **LPNL: Scalable Link Prediction with Large Language Models**.
[[arxiv]](https://arxiv.org/abs/2401.13227) [[paper]](https://arxiv.org/pdf/2401.13227.pdf)

-	Lingrui Mei, Shenghua Liu, Yiwei Wang, `Baolong Bi`, Xueqi Cheng. **SLANG: New Concept Comprehension of Large Language Models**.
[[arxiv]](https://arxiv.org/abs/2401.12585) [[paper]](https://arxiv.org/pdf/2401.12585.pdf) 

</div>

